import streamlit as st
import google.generativeai as genai
import os

# Sayfa AyarlarÄ±
st.set_page_config(page_title="FLL Kural AsistanÄ±", page_icon="ğŸ¤–")

# BaÅŸlÄ±k
st.title("ğŸ¤– FLL Submerged - Kural AsistanÄ±")
st.write("FLL kurallarÄ± ve gÃ¶revleri hakkÄ±nda sorularÄ±nÄ±zÄ± sorun.")

# 1. API KEY AYARI (GÃ¼venlik iÃ§in Secrets'tan Ã§ekeceÄŸiz)
# GitHub'a asla aÃ§Ä±k API Key yÃ¼klemeyin!
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
except:
    st.error("API Key bulunamadÄ±! LÃ¼tfen Streamlit Secrets ayarlarÄ±nÄ± yapÄ±n.")
    st.stop()

# 2. MODEL VE TALÄ°MATLAR
# Buraya AI Studio'daki "System Instruction" metnini yapÄ±ÅŸtÄ±rÄ±n.
# EÄŸer PDF kullandÄ±ysan, PDF iÃ§eriÄŸini metne dÃ¶kÃ¼p buraya eklemek en garanti yoldur.
SYSTEM_PROMPT = """
Sen uzman bir FIRST LEGO League (FLL) BaÅŸhakemisin. 
SorularÄ± yanÄ±tlarken FLL Robot Oyunu kural kitapÃ§Ä±ÄŸÄ±nÄ± referans al.
Daima nazik ve Ã¶ÄŸretici ol. CevaplarÄ±nda kural maddelerini (R12, M04 gibi) belirt.
"""

# Modeli BaÅŸlat
model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    system_instruction=SYSTEM_PROMPT
)

# 3. SOHBET GEÃ‡MÄ°ÅÄ° YÃ–NETÄ°MÄ°
if "messages" not in st.session_state:
    st.session_state.messages = []

# Eski mesajlarÄ± ekrana Ã§iz
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 4. KULLANICI GÄ°RÄ°ÅÄ° VE CEVAP
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Bot cevabÄ±nÄ± Ã¼ret
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # Sohbet geÃ§miÅŸini modele gÃ¶nder
        chat = model.start_chat(history=[
            {"role": m["role"], "parts": [m["content"]]} 
            for m in st.session_state.messages[:-1]
        ])
        
        response = chat.send_message(prompt, stream=True)
        
        # AkÄ±ÅŸkan (streaming) cevap efekti
        for chunk in response:
            full_response += chunk.text
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "model", "content": full_response})
